{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "\n",
    " 1. The traditional PCA and the LDA algorithms works pretty good with the data that is linearly seperable but fails   to work in case when the data points are not linearly seperable.\n",
    "<img src=\"./images/graph_1.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    " 2. This is when **Kernel PCA** comes handy(which relates to the concept of kernel SVM).\n",
    " 3. We are performing a non-linear mapping via Kernel PCA of the data to higher dimensional space. We then use traditional PCA to transform the data into lower dimension where the data points can be linearly seperated.\n",
    " 4. One downside of this approach is that it is computationally very expensive, thus we use **kernel tricks**, that let's us explore the similarities between the 2 higher dimensional features in its original feature space.\n",
    " 5. A **Kernel Function** can be understood as a function that calculated dot product between 2 vectors - i.e. the measure of similarity.\n",
    " 6. Different Kernel Function encompass :-\n",
    "   * Polynomial Kernel : $k(x^{i},x^{j})=(x^{(i)^T}x^{(j)}+\\theta)^p$ , where $\\theta$ is the threshold and p is the power\n",
    "   * Hyperbolic tangent\n",
    "   * Radial Bias Function(RBF) or **Gaussian Kernel** : $k(x^{i},x^{j})=exp\\bigl(-\\frac{||x^{(i)}-x^{(j)}||^2}{2\\sigma^2}\\bigr)$, where $\\gamma=\\frac{1}{2\\sigma^2}$\n",
    " 7. **Steps involved** :\n",
    "   1. We first compute the **kernel similarity matrix K**, where we compute the dot product for each and every pair of data points.\n",
    "   $$K=\n",
    "    \\begin{pmatrix}\n",
    "    k(x^{(1)},x^{(1)}) & k(x^{(1)},x^{(2)}) & \\cdots & k(x^{(1)},x^{(n)}) \\\\\n",
    "    k(x^{(2)},x^{(1)}) & k(x^{(2)},x^{(2)}) & \\cdots & k(x^{(2)},x^{(n)}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    k(x^{(n)},x^{(1)}) & k(x^{(n)},x^{(2)}) & \\cdots & k(x^{(n)},x^{(n)})\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "  2. In PCA we standardize the data such that the mean for the data points is 0, in this case as we project the data onto higher dimension there is no assurance that the data points will be standardize, hence its important to standardize the data. $K=K - 1_nK - K1_n - 1_nK1_n$ , where $1_n$ is the n x n matrix with values 1/n.\n",
    "  3. We collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, which are ranked by decreasing magnitude. In contrast to standard PCA, the eigenvectors are not the principal component axes, but the samples already projected onto these axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist,squareform\n",
    "from scipy import exp\n",
    "from scipy.linalg import eigh\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_pca(X,gamma,n_components):\n",
    "    \"\"\"\n",
    "    RBF kernel PCA implementation.\n",
    "    Parameters\n",
    "    ------------\n",
    "    X: {NumPy ndarray}, shape = [n_samples, n_features]\n",
    "        gamma: float\n",
    "          Tuning parameter of the RBF kernel\n",
    "        n_components: int\n",
    "          Number of principal components to return\n",
    "    Returns ------------\n",
    "    X_pc: {NumPy ndarray}, shape = [n_samples, k_features] Projected dataset\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculate the pairwise squared Euclidean distances in the MxN dimensional dataset\n",
    "    # implements : sum((x[0]-x[1])**2)\n",
    "    sq_dist=pdist(X,'sqeuclidean')\n",
    "    \n",
    "    \n",
    "    # Converts pariwise distances into a square martix.\n",
    "    # i.e. distance x[i]-x[j] will be placed at position (i,j)\n",
    "    mat_sq_dists=squareform(sq_dist)\n",
    "    \n",
    "    # Compute the symmetric kernel matrix\n",
    "    K=np.exp(-gamma*mat_sq_dists)\n",
    "    \n",
    "    # Standardizing the new data points\n",
    "    N=K.shape[0]\n",
    "    \n",
    "    one_n=np.ones((N,N))/N\n",
    "    K=K - one_n.dot(K) - K.dot(one_n) - (one_n.dot(K)).dot(one_n)\n",
    "    \n",
    "    # obtaining eigen pairs from the centered kernel matrix\n",
    "    # Diff between eig and eigh is that eigh return sorted eigen values in ascending order\n",
    "    # and should be used in case when the input matrix is symmetric\n",
    "    eigen_vals,eigen_vecs=np.linalg.eigh(K)\n",
    "    \n",
    "    # Rearranging in descending order\n",
    "    eigen_vals,eigen_vecs=eigen_vals[::-1],eigen_vecs[:,::-1]\n",
    "    \n",
    "    # Collect top k eigen vectors\n",
    "    X_pc=np.column_stack((eigen_vecs[:,i] for i in range(n_components)))\n",
    "    \n",
    "    return X_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\gamma$ needs to be give in advance and the best way to choose the value for $\\gamma$ is via **HyperParameter tunning Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
