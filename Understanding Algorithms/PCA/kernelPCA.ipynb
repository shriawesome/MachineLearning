{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "\n",
    " 1. The traditional PCA and the LDA algorithms works pretty good with the data that is linearly seperable but fails   to work in case when the data points are not linearly seperable.\n",
    "<img src=\"./images/graph_1.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    " 2. This is when **Kernel PCA** comes handy(which relates to the concept of kernel SVM).\n",
    " 3. We are performing a non-linear mapping via Kernel PCA of the data to higher dimensional space. We then use traditional PCA to transform the data into lower dimension where the data points can be linearly seperated.\n",
    " 4. One downside of this approach is that it is computationally very expensive, thus we use **kernel tricks**, that let's us explore the similarities between the 2 higher dimensional features in its original feature space.\n",
    " 5. A **Kernel Function** can be understood as a function that calculated dot product between 2 vectors - i.e. the measure of similarity.\n",
    " 6. Different Kernel Function encompass :-\n",
    "   * Polynomial Kernel : $k(x^{i},x^{j})=(x^{(i)^T}x^{(j)}+\\theta)^p$ , where $\\theta$ is the threshold and p is the power\n",
    "   * Hyperbolic tangent\n",
    "   * Radial Bias Function(RBF) or **Gaussian Kernel** : $k(x^{i},x^{j})=exp\\bigl(-\\frac{||x^{(i)}-x^{(j)}||^2}{2\\sigma^2}\\bigr)$, where $\\gamma=\\frac{1}{2\\sigma^2}$\n",
    " 7. **Steps involved** :\n",
    "   1. We first compute the **kernel similarity mxtrix K**, where we compute the dot product for each and every pair of data points.\n",
    "   $$K=\n",
    "    \\begin{pmatrix}\n",
    "    k(x^{(1)},x^{(1)}) & k(x^{(1)},x^{(2)}) & \\cdots & k(x^{(1)},x^{(n)}) \\\\\n",
    "    k(x^{(2)},x^{(1)}) & k(x^{(2)},x^{(2)}) & \\cdots & k(x^{(2)},x^{(n)}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    k(x^{(n)},x^{(1)}) & k(x^{(n)},x^{(2)}) & \\cdots & k(x^{(n)},x^{(n)})\n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "  2. In PCA we standardize the data such that the mean for the data points is 0, in this case as we project the data onto higher dimension there is no assurance that the data points will be standardize, hence its important to standardize the data. $K=K - 1_nK - K1_n - 1_nK1_n$ , where $1_n$ is the n x n matrix with values 1/n.\n",
    "  3. We collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, which are ranked by decreasing magnitude. In contrast to standard PCA, the eigenvectors are not the principal component axes, but the samples already projected onto these axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
