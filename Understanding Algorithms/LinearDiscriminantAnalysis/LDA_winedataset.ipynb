{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis(LDA)\n",
    "LDA can be used for Feature Extraction that can help in increasing the computational efficiency and reduce the degree of overfitting due to the curse of dimensionality. <br>\n",
    "\n",
    "Unlike PCA,the goal in LDA is to find the feature space that optimizes class separability and also unlike PCA it is a type of Supervised learning algorithm.\n",
    "![](./images/LDA.png)\n",
    "\n",
    "A linear discriminant, as shown on the x-axis (LD 1), would separate the two normal distributed classes well. Although the exemplary linear discriminant shown on the y- axis (LD 2) captures a lot of the variance in the dataset, it would fail as a good linear discriminant since it does not capture any of the class-discriminatory information.\n",
    "<br><br>\n",
    "**Assumptions in LDA**\n",
    "1. The data needs to be normally distributed.\n",
    "2. The Classes have identical covariance matrices.\n",
    "3. Features are statistically independent of each other.\n",
    "\n",
    "**Steps involved in LDA**\n",
    "1. Standardize the d-dimensional dataset.\n",
    "2. For each class, compute the d-dimensional mean vector.\n",
    "3. Construct the between-class scatter martix $S_{B}$ and the within-class scatter matrix $S_{w}$\n",
    "4. Compute the eigenvectors and corresponding eigenvalues of the matrix $S_{w}^{-1}S_{B}$.\n",
    "5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "6. Choose the k-eigenvectors to transform the data.\n",
    "7. Project the samples onto a new feature subspace using the transformation matrix **W**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       "0   1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1   1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2   1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3   1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4   1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('https://archive.ics.uci.edu/ml/' 'machine-learning-databases/wine/wine.data', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into 80% training and 20% testing\n",
    "X,y=df.iloc[:,1:].values, df.iloc[:,0].values\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "x_train_std=scaler.fit_transform(x_train)\n",
    "x_test_std=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing the scatter matrices\n",
    "### 2.1 Mean Vectors\n",
    "Before jumping to scatter matrices we need to find the mean vector, which will then be used to construct scatter matrix and between-class scatter matrix.<br> Mean Vector :-\n",
    "$$ \\left[\n",
    "\\begin{array}\n",
    "  $\\mu_{i,alcohol} \\\\\n",
    "  \\mu_{i,malic acid} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\mu_{i,proline}\n",
    "\\end{array}\n",
    "\\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MV 1: [ 0.91978353 -0.30745801  0.31170497 -0.77070128  0.3570667   0.94244036\n",
      "  1.08126971 -0.61091043  0.5657055   0.19023193  0.5023533   0.82020082\n",
      "  1.19673935]\n",
      "MV 2: [-0.89419108 -0.4180267  -0.46748599  0.22143613 -0.29257833 -0.07476381\n",
      "  0.03134406 -0.05767259  0.10113134 -0.89096913  0.46438407  0.29201464\n",
      " -0.70869344]\n",
      "MV 3: [ 1.85482156e-01  8.76835219e-01  2.78214421e-01  5.35775485e-01\n",
      "  5.66825020e-04 -9.11852530e-01 -1.19954906e+00  7.30070523e-01\n",
      " -7.38546934e-01  9.62925373e-01 -1.14635768e+00 -1.26118672e+00\n",
      " -3.54169803e-01]\n"
     ]
    }
   ],
   "source": [
    "mean_vecs=[]\n",
    "\n",
    "# 3 different labels in the dataset\n",
    "for label in range(1,4):\n",
    "    mean_vecs.append(np.mean(x_train_std[y_train==label],axis=0))\n",
    "    print('MV {}: {}'.format(label,mean_vecs[label-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Within-Class scatter matrix:\n",
    "![](./images/SM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vecs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within-class scatter matrix: 13x13\n"
     ]
    }
   ],
   "source": [
    "d=13 # number of features\n",
    "S_W=np.zeros((d,d))\n",
    "for label,mv in zip(range(1,4),mean_vecs):\n",
    "    class_scatter=np.zeros((d,d))\n",
    "    for row in x_train_std[y_train==label]:\n",
    "        row,mv=row.reshape(d,1),mv.reshape(d,1)\n",
    "        class_scatter+=(row-mv).dot((row-mv).T)\n",
    "    S_W+=class_scatter\n",
    "print('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the Sw by number of classes gives us the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Alternative for the inner for loop( Vectorization)\n",
    "# (x_train_std[y_train==1] - mean_vecs[0]).T.dot(x_train_std[y_train==1] - mean_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "There is a basic assumption that the class labelled data is uniformly distributed but this is not the case.\n",
    "Thus we need to scale the individual scatter matrix before we want to sum them up, and when we do that we are actually evaluating the Covariance matrix.<br>\n",
    " $$\\sum_{i}=\\frac{1}{n_{i}}S_{w}=\\frac{1}{n_{i}}\\sum_{}(x-m_i)(x-m_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 55, 42])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is a basic assumption that the class labelled data is uniformly distributed but this is not the case.\n",
    "# Thus we need to scale the individual scatter matrix before we want to sum them up, and when we do that we are actually \n",
    "# evaluating the Covariance matrix.\n",
    "np.bincount(y_train)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Scaled within class Scatter matrix : (13, 13)\n"
     ]
    }
   ],
   "source": [
    "S_W=np.zeros((d,d))\n",
    "for label in range(1,4):\n",
    "    class_scatter=np.cov(x_train_std[y_train==label].T)\n",
    "    S_W+=class_scatter\n",
    "print('Shape of Scaled within class Scatter matrix :',S_W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Between class scatter matrix:\n",
    "$$S_B=\\sum_{i=1}^{c}n_i(m_i-m)(m_i-m)^T$$\n",
    "where, m = mean of the entire dataset and $m_i$ is the mean of the class label i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between class Scatter matrix shape :  (13, 13)\n"
     ]
    }
   ],
   "source": [
    "mean_overall=np.mean(x_train_std,axis=0)\n",
    "S_B=np.zeros((d,d))\n",
    "for i,mean_vec in enumerate(mean_vecs):\n",
    "    n=x_train_std[y_train==i+1].shape[0]\n",
    "    mean_vec=mean_vec.reshape(d,1)\n",
    "    mean_overall=mean_overall.reshape(d,1)\n",
    "    S_B+=n*(mean_vec-mean_overall).dot((mean_vec-mean_overall).T)\n",
    "print('Between class Scatter matrix shape : ',S_B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Selecting Linear Discriminants for the new feature subspace\n",
    "The rest of the steps are similar to PCA but instead of computing eigen values and eigen vectors on Covariance matrix we'll be solving $S_w^{-1}S_B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen values in descending order:\n",
      "544.9409138254811\n",
      "187.26591131223125\n",
      "4.716069852360453e-14\n",
      "2.986845217823087e-14\n",
      "2.842170943040401e-14\n",
      "2.4902697537481495e-14\n",
      "2.4902697537481495e-14\n",
      "2.4133016351395774e-14\n",
      "2.0867089155205042e-14\n",
      "1.8804853358358074e-14\n",
      "1.8804853358358074e-14\n",
      "2.8803848673629816e-15\n",
      "2.8803848673629816e-15\n"
     ]
    }
   ],
   "source": [
    "eigen_vals,eigen_vecs=np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "eigen_pairs=[(np.abs(eval),evec) for eval,evec in zip(eigen_vals,eigen_vecs)]\n",
    "eigen_pairs=sorted(eigen_pairs,key=lambda k:k[0],reverse=True)\n",
    "print('Eigen values in descending order:')\n",
    "for eigen_val in eigen_pairs:\n",
    "    print(eigen_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eigen_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "Need to calculate the **variance explained ratios** Given by :-\n",
    "$$\\frac{\\lambda_j}{\\sum_{j=1}^d\\lambda_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tot_sum=np.sum(eigen_vals.real)\n",
    "discr=[i/tot_sum for i in sorted(eigen_vals.real,reverse=True)]\n",
    "cum_sum=np.cumsum(discr)\n",
    "plt.bar(range(1,14),discr,align='center',label='Individual Variance explained')\n",
    "plt.step(range(1,14),cum_sum,label='Cumulative Variance Explained')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('# Features')\n",
    "plt.ylabel('Expalined Variance')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix W :  [[-0.24153337 -0.12924145]\n",
      " [-0.38105086 -0.23564322]\n",
      " [-0.03002824  0.06602929]\n",
      " [ 0.08106218  0.40246235]\n",
      " [ 0.17619901  0.11310164]\n",
      " [ 0.1564624   0.02484698]\n",
      " [ 0.1564624   0.02484698]\n",
      " [-0.26092788 -0.08538147]\n",
      " [ 0.13557358  0.32076021]\n",
      " [-0.19246088 -0.17732492]\n",
      " [-0.19246088 -0.17732492]\n",
      " [-0.13411033  0.00096329]\n",
      " [-0.13411033  0.00096329]]\n"
     ]
    }
   ],
   "source": [
    "# let's stack the two most discriminative eigen vectors\n",
    "W=np.hstack((eigen_pairs[0][1][:,np.newaxis].real,eigen_pairs[1][1][:,np.newaxis].real))\n",
    "print('Matrix W : ',W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Projecting samples onto new feature space\n",
    "$$X^\"=XW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
